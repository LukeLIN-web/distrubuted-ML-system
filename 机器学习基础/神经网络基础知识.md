

[《动手学深度学习》 — 动手学深度学习 2.0.0-alpha2 documentation (d2l.ai)](https://zh-v2.d2l.ai/)



```python
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)  # `X`和`y`的小批量损失
        # 因为`l`形状是(`batch_size`, 1)，而不是一个标量。`l`中的所有元素被加到一起，
        # 并以此计算关于[`w`, `b`]的梯度
        l.sum().backward()
        sgd([w, b], lr, batch_size)  # 使用参数的梯度更新参数
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')
```

```python
from torch.utils import data
net = nn.Sequential(nn.Linear(2, 1)) #封装好, list of layers
```

### 4.4.2.1. 验证集

原则上，在我们确定所有的超参数之前，我们不应该用到测试集。如果我们在模型选择过程中使用测试数据，可能会有过拟合测试数据的风险。那我们就麻烦大了。如果我们过拟合了训练数据，还有在测试数据上的评估来判断过拟合。但是如果我们过拟合了测试数据，我们又该怎么知道呢？

因此，我们决不能依靠测试数据进行模型选择。然而，我们也不能仅仅依靠训练数据来选择模型，因为我们无法估计训练数据的泛化误差。

在实际应用中，情况变得更加复杂。虽然理想情况下我们只会使用测试数据一次，以评估最好的模型或比较一些模型效果，但现实是，测试数据很少在使用一次后被丢弃。我们很少能有充足的数据来对每一轮实验采用全新测试集。

解决此问题的常见做法是将我们的数据分成三份，除了训练和测试数据集之外，还增加一个*验证数据集*（validation dataset），也叫*验证集*（validation set）。但现实是验证数据和测试数据之间的边界模糊得令人担忧。除非另有明确说明，否则在这本书的实验中，我们实际上是在使用应该被正确地称为训练数据和验证数据的东西，并没有真正的测试数据集。因此，书中每次实验报告的准确度都是验证集准确度，而不是测试集准确度。

### 4.4.2.2. KK折交叉验证

当训练数据稀缺时，我们甚至可能无法提供足够的数据来构成一个合适的验证集。这个问题的一个流行的解决方案是采用KK*折交叉验证*。这里，原始训练数据被分成KK个不重叠的子集。然后执行KK次模型训练和验证，每次在K−1K−1个子集上进行训练，并在剩余的一个子集（在该轮中没有用于训练的子集）上进行验证。最后，通过对KK次实验的结果取平均来估计训练和验证误差。





### DNN

1. We pass the data through the layers of the DNN to compute the loss (i.e., forward pass)

2. We back-propagate the loss through every layer to compute the gradients (i.e., backward pass)

3. We apply a learning rule to update the parameters of the model (i.e., optimization step)







NN 的基础知识

1. SGD优化器 可以看SGD 的介绍 :Pytorch中常用的四种优化器SGD、Momentum、RMSProp、Adam - https://zhuanlan.zhihu.com/p/78622301
2. 通过Softmax函数就可以将多分类的输出值转换为范围在 [0, 1]和为1的概率分布
3. batch  变大, 梯度方差变小, 容易停留在不太好的局部最优.  而且容易收敛到尖锐的局部最优, 泛化性差.   人们常用的方法是训练初期模型更新比较大的时候使用小的学习率, 然后慢慢把学习率调大.

### attention

特殊的加权. 

对于一段文本序列，我们通常要使用某种机制对该序列进行编码，通过降维等方式将其encode成一个固定长度的向量，用于输入到后面的全连接层。

一般我们会使用CNN或者RNN（包括GRU或者sLSTM）等模型来对序列数据进行编码，然后采用各种pooling或者对RNN直接取最后一个t时刻的hidden state作为句子的向量输出。

欸, 我觉得这个很适合动态job的情况,新来了job那就可以变化.   (事实上谷歌大脑就是LSTM来做的.  但是yixin bao 没有搞懂他们是怎么做的, 估计难度比较大. )

在本节中，我们正式定义了问题和我们提出的框架。让我们考虑一个给定的输入集合x＝{x i，i＝1，···，m }的席夫优化问题。我们允许每个输入的某些元素在解码步骤之间发生变化，这实际上是许多问题（如VRP）的情况。动态元素可能是解码过程本身的产物，也可能是由环境强加的。
例如，在VRP中，当车辆访问客户节点时，剩余的客户需求随着时间的推移而改变；或者我们可以考虑一种变体，其中新客户到达或调整他们的需求值随着时间的推移，独立于车辆的决策。形式上，我们用元组序列{xit=（si，di t），t=0，···，t}来表示每个输入x i，其中si和dit分别是输入的静态和动态元素，它们本身可以是元组。例如，在VRP中，si对应于客户i的位置的二维坐标，dit是其在时间t的需求。我们将用Xt表示在固定时间t的所有输入状态的集合。

